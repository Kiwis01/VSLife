{
    "resume": "CARLOS QUIHUIS\n+1 (520) 312-8154  | chquihui@asu.edu  | Tempe, AZ, USA  | linkedin.com/in/carlos-quihuis-190b431aa \nEDUCATION\n \n \nArizona State University\nAugust 2020 - May 2025\nBachelor's, Computer Science\nGPA: 3.52\nPROFESSIONAL EXPERIENCE\n \n \nCity of Phoenix\nPhoenix, AZ, USA\nSoftware Engineering Intern\nSeptember 2024 - Present\n\u2022  \nOptimized procedures of data backup retrieval from the database\n\u2022  \nCoordinated queries to replicate the hierarchy of the assets for database migration\n\u2022  \nIdentification of items efficiency increased by 80%\n\u2022  \nOrganized assets in series with corresponding operations and maintenance manuals\nSAGO\nPhoenix, AZ, USA\nIT Technician\nSeptember 2023 - Present\n\u2022  \nIntegrated automated processes for data uploading to the cloud\n\u2022  \nConducted troubleshooting and maintenance for the server on-site\n\u2022  \nUpgraded hardware and software gaining higher resolution for AV recordings (480p to 1080p)\n\u2022  \nEngineered web application to gain AV system control directly from iPads\nPROJECTS & OUTSIDE EXPERIENCE\n \n \nHealthy\n\u2022  \nDeveloped software to reconstruct MRI scans into 3D models, improving diagnostic efficiency\n\u2022  \nIntegrated direct connectivity to PACS server for seamless DICOM file access\n\u2022  \nEnabled a medical image visualization in virtual and augmented reality for enhanced analysis\n\u2022  \nEngineered a data integration solution to consolidate multiple data sources (database, excel sheets, call center system)\n\u2022  \nLink to project\nMedAI\n\u2022  \nDeveloped a smart agent for diagnosing brain illnesses\n\u2022  \nEngineered a neural network for brain tumor classification, predicting Glioma, Meningioma, and Pituitary from medical \nimages\n\u2022  \nBuilt Retrieval Augmented Generation model to provide a chatbot data retrieval and text generation\n\u2022  \nLink to project\nVerso\n\u2022  \nBuilt a RESTful application enabling user interaction with large language models (LLMs)\n\u2022  \nIntegrated OpenAI API for natural language processing and langchain for embeddings\n\u2022  \nEnabled PDF upload and context document integration for smarter conversations\n\u2022  \nLink to project\nPartition Reader\n\u2022  \nDeveloped software to identify partition allocation\n\u2022  \nFacilitated the extraction of globally unique identifiers and logical block addresses (hex format)\n\u2022  \nUser can specify offsets for retrieval of 16 bytes addresses\n\u2022  \nMaintained consistent runtimes <2.3 seconds runtime\n\u2022  \nLink to project\nSKILLS\n \n \nSkills: Python, Git, Java, HTML/CSS, Excel/Numbers/Sheets, machine learning, React.js, Blockchain, C#, C/C++, Data Analysis, \nData Structures & Algorithms, Flask, Firebase, JavaScript, LangChain, Linux/Unix, .NET, Pandas, Pytorch, REST APIs, Tensorflow, \nBash, Node.js, AWS, SQL, TypeScript, Docker, Pandas, forecasting, end-to-end\nLanguages: French, Italian, Spanish",
    "github_repos": [
        {
            "name": "KiwisGPT",
            "description": "AI chatbot with my personal info as reference for any questions regarding personal experiences",
            "language": "JavaScript",
            "readme": "KiwisGPT - Personalized AI Chatbot KiwisGPT is an AI chatbot designed to answer questions based on my personal experiences and knowledge. This repository contains the necessary code to run the chatbot and get personalized responses.\nInstallation\nTo get started, make sure you have Node.js installed on your machine. Then, navigate to the project directory and run the following command to install the required dependencies:\n\"npm install\"\nSetup API Key\nTo use KiwisGPT, you will need a private API key from the OpenAI website. Once you obtain the API key, create a .env file in the root directory of the project and store the API key in a variable named APIKEY like this:\n\"APIKEY=YOUR_API_KEY_HERE\" Ensure that you keep your API key private and do not share it publicly.\nUsage\nOnce you have installed the dependencies and set up your API key, you are ready to use KiwisGPT. There are two ways to run the chatbot:\nLocal Development:\nYou can run the chatbot on your local machine using a live server. To do this, execute the following command: \"npm start\" The chatbot will be accessible at localhost or the specified host and port.\nDeployment:\nIf you wish to deploy the chatbot on another web server, build the project using:\n\"npm run build\" This will generate a production-ready build in the dist folder. You can then deploy the contents of this folder to your desired web server.\nSupport and Contribution\nFeel free to explore, use, and modify this project as needed. If you encounter any issues or have suggestions for improvement, please create an issue on this repository. Contributions are also welcome through pull requests.",
            "url": "https://github.com/Kiwis01/KiwisGPT",
            "created_at": "2023-07-08T05:33:43Z",
            "updated_at": "2024-12-14T09:14:50Z",
            "pushed_at": "2023-08-22T05:57:59Z",
            "stargazers_count": 1,
            "forks_count": 0
        },
        {
            "name": "Partition-Reader",
            "description": "This code will read the partition, classify its type and parse in the partition tables based on the offset",
            "language": "Python",
            "readme": "Partition-Reader\nThis code was made with standard python libraries, imports are \"argparse, hashlib, os, json\". To run the code you can execute the makefile by entering \"make\" on the terminal or referencing the makfile directly \"make makefile\". Then an executable \"./boot_info\" will be available to use.\nUsage\n\"./boot_info -f file.raw -o 123 73 234\" --f {filename} --o {offsets}\nDescription\nThis code can calculate MD5 hash and SHA 256 SHA hash values and store them into a designated txt file with the name of the raw file in between the hash and .txt, ex. \"MD5-{filename}.txt\".\nAfter that the code will check if the entered file -f has a GPT or MBR partition, then it will parse different partition trees depending on that.\nIf it detects the ID byte is \"ee\" or \"EE\" then it will parse a GPT partition tree, otherwise, it will be an MBR tree.\nGPT tree shows the partitions GUID type, First and Last LBA address in both hex and decimal values. As well as the Partition name decoded to Romani characters (ASCII).\nMBR tree shows the ID of the partition and extracts the type of partition by comparing the JSON file containing each ID and their corresponding partition, then it also prints the 16 bytes starting from the offset that was entered in the -o argument (For this we used the LBA * 512 + offsets to find the 16 bytes in the offset regarding the position of the start LBA address). It also outputs the ASCII translation of the 16 bytes.\nTesting\nI uploaded two \"raw\" files for testing purposes, feel free to use any file you want.",
            "url": "https://github.com/Kiwis01/Partition-Reader",
            "created_at": "2024-03-20T21:27:47Z",
            "updated_at": "2024-03-23T05:58:39Z",
            "pushed_at": "2024-03-20T21:52:44Z",
            "stargazers_count": 1,
            "forks_count": 0
        },
        {
            "name": "Spotipy",
            "description": "Python executable application that allows the user play, pause and skip song on spotify, utilizing spotify api",
            "language": "Python",
            "readme": "Spotipy\nPython executable application that allows the user play, pause and skip song on spotify, utilizing spotify api\nImports installation\nCopy and paste this command on the terminal to install the neccesary package imports \"pip install -r requirements.txt\"\nSetup and Environmental variables\nMake an .env file where you will have two variables 'CLIENT_ID' and 'CLIENT_SECRET', you can get these values from your spotify app https://developer.spotify.com/dashboard You also have to make sure that you have a redirect uri set up in the spotify app settings, I used 'http://localhost:8888/callback' but you can change the port and '/callback' to something else, just make sure it matches the redirect uri set in the code.\nRun\npython main.py",
            "url": "https://github.com/Kiwis01/Spotipy",
            "created_at": "2023-08-14T05:45:25Z",
            "updated_at": "2024-07-11T21:05:31Z",
            "pushed_at": "2023-08-14T10:55:22Z",
            "stargazers_count": 1,
            "forks_count": 0
        },
        {
            "name": "bchoc",
            "description": "my end",
            "language": "Python",
            "readme": "Blockchain Project\nNeed to install pycryptodome using pip install pycryptodome\nRun chmod +x on bchoc.py to run as executable for testing, otherwise use make",
            "url": "https://github.com/Kiwis01/bchoc",
            "created_at": "2024-04-15T18:31:19Z",
            "updated_at": "2024-04-15T18:32:02Z",
            "pushed_at": "2024-04-15T18:31:59Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "BoredActivities",
            "description": "Simple html page that will let the user generate activities to kill boredom",
            "language": "JavaScript",
            "readme": "BoredActivities\nEnjoy :)",
            "url": "https://github.com/Kiwis01/BoredActivities",
            "created_at": "2023-12-09T21:51:00Z",
            "updated_at": "2023-12-10T19:03:22Z",
            "pushed_at": "2023-12-23T06:59:51Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "Brain-Tumor-CNN",
            "description": "Trained and layered Convolutional Neural Network for brain tumor classification and detection",
            "language": "Jupyter Notebook",
            "readme": "Brain-Tumor-CNN\nTrained and layered Convolutional Neural Network for brain tumor classification and detection\nInstallation\npip install -r requirements.txt\nDescription\nImported dataset from kaggle, containing four different classification of images: glioma, meningioma, notumor, pituitary Utilized the Convolution2D, MaxPooling, and Batch Normalization.",
            "url": "https://github.com/Kiwis01/Brain-Tumor-CNN",
            "created_at": "2024-12-01T09:48:56Z",
            "updated_at": "2024-12-01T10:16:09Z",
            "pushed_at": "2024-12-01T10:16:06Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "CoffeeRecipes",
            "description": "Website that will make recipes for any coffee you would like using AI",
            "language": "CSS",
            "readme": "Files\n main\napi\nstatic\ntemplates\nprompt.txt\nrequirements.txt\nvercel.json\nBreadcrumbs\nCoffeeRecipes\n/README.md/\n404 - page not found\nThe \nmain\n branch of \nCoffeeRecipes\n does not contain the path \nREADME.md.\nReturn to the repository overview",
            "url": "https://github.com/Kiwis01/CoffeeRecipes",
            "created_at": "2023-08-01T00:21:25Z",
            "updated_at": "2023-08-02T21:35:52Z",
            "pushed_at": "2023-08-02T21:43:01Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "Expenses",
            "description": "Personal Expenses Calculator",
            "language": "HTML",
            "readme": "Expenses\nPersonal Expenses Calculator",
            "url": "https://github.com/Kiwis01/Expenses",
            "created_at": "2024-12-14T08:54:07Z",
            "updated_at": "2025-01-12T19:47:32Z",
            "pushed_at": "2025-01-12T19:47:30Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "FirebaseTest",
            "description": null,
            "language": "HTML",
            "readme": "FirebaseTest\n\"npm init -y\" \"npm install firebaseui --save\" \"python server.py\"",
            "url": "https://github.com/Kiwis01/FirebaseTest",
            "created_at": "2024-05-14T15:08:30Z",
            "updated_at": "2024-06-14T04:15:26Z",
            "pushed_at": "2024-06-14T04:15:22Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "Kiwis01",
            "description": null,
            "language": null,
            "readme": "Hi \ud83d\udc4b!, I'm Carlos Quihuis\n\ud83d\udcbb I build scalable software powered by AI, LLMs, and blockchain.\n\ud83e\udde0 I\u2019ve built medical AI tools like brain tumor classifiers and interactive 3D anatomical models.\n\ud83d\udd2c Currently exploring Transformers and Large Language Models to ship scalable, intelligent systems.\n\ud83d\ude80 Always prototyping. Always learning. Always building.",
            "url": "https://github.com/Kiwis01/Kiwis01",
            "created_at": "2025-05-06T20:50:46Z",
            "updated_at": "2025-05-06T21:27:09Z",
            "pushed_at": "2025-05-18T01:03:23Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "LandingBoston",
            "description": "Landing Page Version for Boston",
            "language": "HTML",
            "readme": "Files\n main\nassets\ncss\nCNAME\nindex.html\nindex.js\npackage.json\nreadme.md\ntailwind.config.js\nBreadcrumbs\nLandingBoston\n/README.md/\n404 - page not found\nThe \nmain\n branch of \nLandingBoston\n does not contain the path \nREADME.md.\nReturn to the repository overview",
            "url": "https://github.com/Kiwis01/LandingBoston",
            "created_at": "2024-11-07T08:54:26Z",
            "updated_at": "2025-01-17T02:16:38Z",
            "pushed_at": "2025-01-17T02:16:36Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "llama-train",
            "description": null,
            "language": "Python",
            "readme": "llama-train",
            "url": "https://github.com/Kiwis01/llama-train",
            "created_at": "2025-04-21T21:05:37Z",
            "updated_at": "2025-04-26T22:13:32Z",
            "pushed_at": "2025-04-26T22:13:28Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "MadLib",
            "description": "mid",
            "language": "CSS",
            "readme": "Files\n main\nREADME\nindex.html\nscript.js\nstyle.css\nBreadcrumbs\nMadLib\n/README.md/\n404 - page not found\nThe \nmain\n branch of \nMadLib\n does not contain the path \nREADME.md.\nReturn to the repository overview",
            "url": "https://github.com/Kiwis01/MadLib",
            "created_at": "2024-03-21T06:23:05Z",
            "updated_at": "2024-03-21T06:24:51Z",
            "pushed_at": "2024-03-21T06:39:48Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "MedAI",
            "description": "Now contained in docker container ",
            "language": "Python",
            "readme": "\u26d1\ufe0f MedAI\nThis Application is running Flask and Python --version = Python >= 3.8 https://medai-509c7d6aeb86.herokuapp.com\nSetup\nInstall the required dependencies:\npip install -qr requirements.txt \nUseful Information\n\"static/script.js\" handles the files (input image and output predictions).\n\"static/predict\" and \"static/uploads\" handle predicted images and uploaded images, respectively.",
            "url": "https://github.com/Kiwis01/MedAI",
            "created_at": "2025-02-21T20:19:24Z",
            "updated_at": "2025-04-04T07:00:07Z",
            "pushed_at": "2025-04-04T17:24:25Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "Medical-Assistant-AI",
            "description": "Full-stack chatbot mixture of specialists ",
            "language": "Python",
            "readme": "\u2695\ufe0f MedAssistAI\nMedAssistAI is an advanced medical assistant chatbot capable of handling symptom-related issues using a multi-agent AI architecture. The system leverages Google's Gemini LLM to provide specialized medical insights across different domains including cardiology, neurology, and general medicine.\nFeatures\nMulti-Agent Architecture: Routes queries to specialized medical agents based on symptoms\nImage Analysis: Processes medical images (such as MRIs, X-rays) for diagnosis assistance\nResearch Integration: Retrieves relevant medical research for comprehensive responses\nConversation History: Maintains context across user interactions\nCaching System: Optimizes performance by caching predictions and generated images\nTech Stack\nBackend\nFramework: Flask\nLLM: Google Gemini 2.0 Flash\nDatabase: PostgreSQL with SQLAlchemy\nImage Processing: OpenCV\nFrontend\nFramework: React\nUI Library: Material UI\nState Management: React Context API\nSetup Instructions\nPrerequisites\nPython 3.8+\nNode.js and npm\nPostgreSQL database\nGoogle Gemini API key\nInstallation\nYou will need to run two terminals simultaneously:\nTerminal 1 / Run Backend\ncd backend\npython -m venv venv\nsource venv/bin/activate  # On Windows: venv\\Scripts\\activate\npip install -r requirements.txt\nTerminal 2 / Run Frontend\ncd frontend\nnpm install\nConfiguration\nCreate backend/config/config.json with your API keys and database settings (see backend/config/sample_config.json for an example)\nEnsure the PostgreSQL database is created and accessible\nUsage\nTerminal 1 / Start Backend Server\ncd backend\npython app.py\nTerminal 2 / Start Frontend Development Server\ncd frontend\nnpm start\nThe application will be available at http://localhost:3000\nProject Structure\nMedical-Assistant-AI/\n\u251c\u2500\u2500 backend/           # Flask backend with multi-agent architecture\n\u251c\u2500\u2500 frontend/         # React frontend application\n\u2514\u2500\u2500 README.md         # This file\nFor more detailed information:\nBackend Documentation\nFrontend Documentation\nContributors\nCarlos Quihuis - chquihui@asu.edu Jacob Swarzmiller - jswartzm@asu.edu Vincent Nguyen - vnguye58@asu.edu",
            "url": "https://github.com/Kiwis01/Medical-Assistant-AI",
            "created_at": "2025-04-22T10:33:36Z",
            "updated_at": "2025-05-04T10:19:59Z",
            "pushed_at": "2025-05-04T10:19:55Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "Multiplayer-code",
            "description": "Multiplayer leetcode experience to challenge your friends on a harmless competition.",
            "language": "HTML",
            "readme": "Multiplayer-code\nStay tuned for a 1v1 coding problems against your friends",
            "url": "https://github.com/Kiwis01/Multiplayer-code",
            "created_at": "2024-12-02T18:28:41Z",
            "updated_at": "2024-12-24T17:49:39Z",
            "pushed_at": "2024-12-24T17:49:34Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "Nono",
            "description": "Holy Moly",
            "language": "JavaScript",
            "readme": "Files\n main\ncss\nfonts\nimg\njs\nCNAME\nindex.html\nBreadcrumbs\nNono\n/README.md/\n404 - page not found\nThe \nmain\n branch of \nNono\n does not contain the path \nREADME.md.\nReturn to the repository overview",
            "url": "https://github.com/Kiwis01/Nono",
            "created_at": "2023-12-23T06:57:51Z",
            "updated_at": "2023-12-23T06:59:00Z",
            "pushed_at": "2023-12-23T07:02:31Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "PiePayCards",
            "description": "This will be the backend code for PiePay, this is the backend code where all the card transactions will be processed.",
            "language": "JavaScript",
            "readme": "Files\n main\ncards.js\nindex.html\npackage-lock.json\npackage.json\nBreadcrumbs\nPiePayCards\n/README.md/\n404 - page not found\nThe \nmain\n branch of \nPiePayCards\n does not contain the path \nREADME.md.\nReturn to the repository overview",
            "url": "https://github.com/Kiwis01/PiePayCards",
            "created_at": "2023-08-22T02:52:42Z",
            "updated_at": "2023-08-22T03:24:56Z",
            "pushed_at": "2023-08-22T07:13:57Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "PokeApi",
            "description": "Pokemon random generator",
            "language": "JavaScript",
            "readme": "PokeApi\nPokemon random generator",
            "url": "https://github.com/Kiwis01/PokeApi",
            "created_at": "2024-04-16T02:30:26Z",
            "updated_at": "2024-04-16T02:32:53Z",
            "pushed_at": "2024-04-16T02:32:50Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "Portfolio",
            "description": "New portfolio template",
            "language": "HTML",
            "readme": "Portfolio\nNew portfolio template",
            "url": "https://github.com/Kiwis01/Portfolio",
            "created_at": "2024-05-08T21:40:25Z",
            "updated_at": "2024-05-08T21:50:47Z",
            "pushed_at": "2024-05-08T21:50:44Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "Portfolio-react",
            "description": "Portfolio using react ",
            "language": "TypeScript",
            "readme": "This is a Next.js project bootstrapped with create-next-app.\nGetting Started\nFirst, run the development server:\nnpm run dev\n# or\nyarn dev\n# or\npnpm dev\n# or\nbun dev\nOpen http://localhost:3000 with your browser to see the result.\nYou can start editing the page by modifying app/page.tsx. The page auto-updates as you edit the file.\nThis project uses next/font to automatically optimize and load Geist, a new font family for Vercel.\nLearn More\nTo learn more about Next.js, take a look at the following resources:\nNext.js Documentation - learn about Next.js features and API.\nLearn Next.js - an interactive Next.js tutorial.\nYou can check out the Next.js GitHub repository - your feedback and contributions are welcome!\nDeploy on Vercel\nThe easiest way to deploy your Next.js app is to use the Vercel Platform from the creators of Next.js.\nCheck out our Next.js deployment documentation for more details.",
            "url": "https://github.com/Kiwis01/Portfolio-react",
            "created_at": "2025-03-05T04:36:31Z",
            "updated_at": "2025-03-05T04:37:19Z",
            "pushed_at": "2025-03-05T04:37:16Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "Recipient-Organizer",
            "description": "Code utilized to read a pdf (previously an excel sheet) and organize the recipients name and amount paid. ",
            "language": "Python",
            "readme": "Recipient-Organizer\nCode utilized to read a pdf (previously an excel sheet) and organize the recipients name and amount paid.",
            "url": "https://github.com/Kiwis01/Recipient-Organizer",
            "created_at": "2024-02-07T19:58:48Z",
            "updated_at": "2024-02-07T19:59:25Z",
            "pushed_at": "2024-02-07T19:59:20Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "SparkyAI",
            "description": "An AI University assistant leveraging multi-agent architecture with real-time data integration. Combines Self improvement CRAG and Firebase analytics for resource retrieval across courses, shuttles, jobs and much more!",
            "language": "Python",
            "readme": "SparkyAI - University Copilot\nThis repository has been recently made public, the old repository had exposed credential issues.\nA Discord bot designed to assist Arizona State University (ASU) students with access to resources, including news, events, scholarships, courses, and more.\nSparkyAI leverages a complex architecture to deliver accurate and context-aware responses:\nRetrieval-Augmented Generation (RAG): Combines vector search with large language models for precise information retrieval.\nMulti-Agent System: Utilizes specialized AI agents or targeted task execution.\nVector Database: Implements Qdrant for efficient semantic search and document retrieval.\nFeatures\nMaximum Inner Product Search (MIPS): Optimized vector similarity search for large-scale datasets.\nRAPTOR Retrieval: Hierarchical document representation for nuanced information retrieval.\nMulti-step Reasoning: Synthesizes information from multiple sources to answer complex queries.\nDynamic Content Extraction: Combines Selenium-based scraping with AI-powered content refinement.\nAutomatic Version Control: Intelligent document updating based on timestamp comparisons.\nCross-Encoder Reranking: Implements a cross-encoder model to rerank initial retrieval results, improving the relevance of top results.\nHNSWlib Integration: Incorporates a graph-based approach for efficient in-memory searches, complementing existing vector search methods.\nScaNN Integration: Utilizes Google's ScaNN library for scalable and efficient handling of large-scale, high-dimensional vector datasets.\nCustom Embedding Model: Utilizes the BAAI/bge-large-en-v1.5 model for high-quality text embeddings.\nCross-Encoder Reranking: Implements the cross-encoder/ms-marco-MiniLM-L-6-v2 model to rerank initial retrieval results, improving the relevance of top results\nPerformance Optimizations\nBatch Processing: Efficient document storage and retrieval in configurable batches.\nCaching Mechanisms: Implements strategic caching for frequently accessed data.\nAsynchronous Operations: Utilizes asyncio for non-blocking I/O operations.\nRetry Mechanisms: Robust error handling with configurable retry attempts for critical operations.\nMulti-Method Search: Combines RAPTOR, similarity search, MIPS, and ScaNN for comprehensive and efficient information retrieval.\nResult Deduplication: Implements intelligent merging and deduplication of search results from multiple methods.\nAdvanced Architecture\nKey Features\nThe code implements the following key features:\nWeb Scraping & Summarization\nThe ASUWebScraper class handles web scraping of ASU pages:\nUses Selenium for dynamic content and BeautifulSoup for static HTML parsing\nImplements methods for scraping specific ASU resources like course catalogs, library resources, and job postings\nExtracts content using both Selenium-based scraping and Jina AI-powered content extraction\nThe DataPreprocessor class handles summarization:\nCleans and structures scraped text using NLP techniques like tokenization and lemmatization\nUses AI agents (likely the Gemini model) to refine and summarize content\nDiscord Integration\nThe DiscordState class manages the Discord bot's state and interactions:\nInitializes Discord intents for message content and member access\nTracks user information, roles, and channel details\nProvides methods to update and retrieve bot state\nAI-Driven Information Retrieval\nThe system uses a Retrieval-Augmented Generation (RAG) architecture:\nThe VectorStore class manages document storage and retrieval using Qdrant vector database\nImplements semantic search capabilities using HuggingFace embeddings\nThe Utils class contains methods for similarity search and database querying\nThe RAPTOR class reranks and creates a tree of documents\nUtils Class Enhancements\nMulti-Method Search: Orchestrates searches across RAPTOR, similarity, and MIPS methods.\nResult Merging: Intelligently combines and deduplicates results from various search methods.\nGround Source Management: Tracks and manages unique source URLs for comprehensive information retrieval.\nCaching: Implements query and document ID caching for improved performance.\nWebhooks\nThe system uses various sets of custom web scraping functions with search query manipulation to fetch most results-\nThe ASUWebScraper class can be extended to fetch data from different ASU platforms\nDatabase Integration\nThe ASU Discord Bot utilizes two database systems for different purposes:\nVector Store Operations\nQdrant Integration: Utilizes Qdrant for efficient vector storage and retrieval.\nMIPS Search: Implements Maximum Inner Product Search for optimized similarity queries.\nHNSWlib Indexing: Builds and uses HNSW indexes for fast approximate nearest neighbor search.\nAutomatic Index Building: Dynamically constructs search indexes for improved query performance.\nGoogle Sheets Database\nThe GoogleSheet class manages interactions with a Google Sheets database, primarily used for moderator oversight and user tracking.\nKey features:\nUser Management: Stores and retrieves user information, including Discord IDs, names, and email addresses.\nFunction Call Tracking: Increments counters for various bot function calls, allowing moderators to monitor usage patterns.\nData Retrieval: Provides methods to fetch all users or specific user data.\nData Updates: Allows updating user-specific information in the spreadsheet.\nImplementation details:\nUses the Google Sheets API for read and write operations.\nImplements methods like get_all_users(), add_new_user(), and update_user_column().\nProvides error handling and logging for database operations.\nFirestore Database\nThe Firestore class manages interactions with Google's Firestore, used for storing bot-related data and chat histories.\nKey features:\nChat History: Stores complete conversation histories between users and the bot.\nMessage Categorization: Organizes messages by different agent types (action, discord, google, live status, search).\nReal-time Updates: Leverages Firestore's real-time capabilities for instant data synchronization.\nImplementation details:\nInitializes a Firestore client using Firebase Admin SDK.\nImplements methods to update collections, add new messages, and retrieve chat histories.\nStores messages with timestamps and user IDs for comprehensive tracking.\nThe modular architecture allows for easy extension:\nMultiple agent classes (e.g., SuperiorAgent, RAGSearchAgent, ShuttleStatusAgent) can be customized for different tasks\nThe AppConfig class centralizes configuration management, making it easy to add new features\nThe use of asynchronous programming (async/await) throughout the codebase allows for efficient handling of concurrent operations\nTechnologies Used\nAI/ML: Gemini, LangChain, TensorFlow\nNLP: Hugging Face Transformers, NLTK\nEmbeddings: BAAI/bge-large-en-v1.5 model\nCross Encoder: cross-encoder/ms-marco-MiniLM-L-6-v2\nVector Search: Qdrant\nWeb Scraping: Selenium, BeautifulSoup4\nAPIs: Discord API, Firebase API\nDatabases: Firestore, Notion\nAsynchronous Programming: asyncio\nContainerization: Docker\nAgent Descriptions\nName What It Does\nSuperior Agent Handles main messages, decides on direct responses or function calls, and utilizes multiple agents/functions as needed.\nRAG Search Agent Performs search over RAG knowledge base, has also access to general Google search utilityfor queries\nNews Media Agent Access to asu social media and asu news\nShuttle Status Agent access to asu campus rider portal\nDiscord Agent access to discord channels : announcements, feedbacks, customer service, polls, posts,\nLibrary Agent access to library.asu and live rooms status asu\nSports Agent access to sundevilathletics.asu\nStudent Jobs Agent access to workday.asu\nStudent Clubs Events\nAgent access to sundevilsync.asu\naccess to scholarships.asu\nFinetune\nWe are finetuning gemini 1.5 flash model (Superior Agent) with our custom dataset containing 560 examples of different interactions between agents with humans aswell as agents with other agents to increase the accuracy of reasoning aswell as general responses to students:\nCategory Description Proportion (%) Number of Examples\nFactual Questions Questions requiring concise, factual answers, such as \"What are the library hours?\" 23.6% 130\nAction-Based Questions Queries requiring JSON function calls, such as \"Find the latest scholarships.\" 32.7% 180\nHybrid Questions Queries needing reasoning + a function call, such as \"Can you summarize this and get related events?\" 32.7% 180\nJailbreak Commands Edge-case inputs requiring safe acknowledgment or refusal 10.9% 60\nExtensible Design\nGetting Started\nFollow these steps to set up and run the ASU Discord Bot on your local machine.\n1. Clone the Repository\nFirst, clone the repository to your local machine using Git.\ngit clone https://github.com/ashworks1706/SparkyAI.git\ncd sparkyai\n2. Set Up a Virtual Environment\nCreate and activate a Python virtual environment to isolate dependencies.\nOn Linux/Mac:\npython3 -m venv venv\nsource venv/bin/activate\nOn Windows:\npython -m venv venv\nvenv\\Scripts\\activate\n3. Install Dependencies\nInstall all required Python.12.3 packages from the requirements.txt file.\npip install -r requirements.txt\n4. Configure API Keys and Credentials\nThe bot requires several API keys and configuration files to function properly.\nCreate a appConfig.json file in the config/ folder with the following structure Example:\nAdd Firebase API credentials Example:\nDownload the firebase_secret.json file from your Firebase Cloud Console.\nPlace it in the config/ folder.\n5. Set Up Qdrant Vector Database\nThe bot uses Qdrant for vector-based similarity search. Run Qdrant using Docker.\nInstall Docker (if not installed):\nFollow Docker installation instructions for your operating system.\nStart Qdrant:\nRun the following command to start Qdrant on port 6333:\ndocker run -p 6333:6333 -v $(pwd)/qdrant_storage:/qdrant/storage qdrant/qdrant\nNote: On Windows, replace $(pwd) with %cd% or ${PWD} if using PowerShell.\n6. Install Chrome Dependencies (for Web Scraping)\nThe bot uses Selenium for web scraping. Ensure Chrome and Chromedriver are installed.\nOn Linux:\nwget https://dl.google.com/linux/direct/google-chrome-stable_current_amd64.deb \nsudo apt install ./google-chrome-stable_current_amd64.deb \n\nsudo wget https://chromedriver.storage.googleapis.com/index.html?path=133.0.6943.141\nunzip chromedriver_linux64.zip\nsudo mv chromedriver /usr/local/bin/\nsudo chmod +x /usr/local/bin/chromedriver\n\nsudo apt-get install chromium-chromedriver xvfb libxss1 libnss3 libatk1.0-0 libgtk-3-0\nOn Windows:\nDownload and install Google Chrome.\nDownload the appropriate Chromedriver version from Chromedriver Downloads.\nAdd Chromedriver to your system PATH.\n7. Run the Bot\nStart the bot by running the main.py script:\npython main.py\nIf everything is set up correctly, you should see logs indicating that the bot has started successfully.\nTroubleshooting Common Issues\nQdrant Not Running:\nEnsure Docker is installed and running.\nVerify that port 6333 is available on your system.\nCheck Qdrant logs for errors by running:\ndocker logs \nMissing API Keys:\nDouble-check that your config.json file is correctly formatted and placed in the config/ folder.\nEnsure all required keys are present.\nChromedriver Errors:\nMake sure you have installed a compatible version of Chromedriver for your version of Chrome.\nVerify that Chromedriver is in your system PATH.\nDependency Issues:\nIf you encounter dependency errors, try upgrading pip and reinstalling requirements:\npip install --upgrade pip\npip install -r requirements.txt --force-reinstall\nContributing\nContributions are welcome! Please open an issue or submit a pull request for any ideas or improvements.\nCitations\nThis project draws inspiration from and builds upon the following research papers:\nAmagata, D., & Hara, T. (2023). Reverse Maximum Inner Product Search: Formulation, Algorithms, and Analysis. ACM Transactions on the Web, 17(4), 1-23\nSun, P. (2020). Announcing ScaNN: Efficient Vector Similarity Search. Google Research Blog\nGuo, R., Sun, P., Lindgren, E., Geng, Q., Simcha, D., Chern, F., & Kumar, S. (2020). Accelerating Large-Scale Inference with Anisotropic Vector Quantization. International Conference on Machine Learning (ICML)\nDong, W., Moses, C., & Li, K. (2024). SOAR: Improved Indexing for Approximate Nearest Neighbor Search. arXiv preprint arXiv:2404.00774\nKandpal, N., Jiang, H., Kong, X., Teng, J., & Chen, J. (2024). RAPTOR: Recursive Abstractive Processing for Tree-Organized Retrieval. arXiv preprint arXiv:2401.18059v1\nGuo, R., Kumar, S., Choromanski, K., & Simcha, D. (2019). Quantization based Fast Inner Product Search. arXiv preprint arXiv:1509.01469\nThese papers have significantly contributed to the field of vector similarity search, maximum inner product search (MIPS), and efficient indexing techniques, which are fundamental to this project's approach.\nLicense\nThis project is licensed under the MIT License.\nContact\nAuthor: Ash\nPortfolio: ashworks.dev\nLinkedIn: linkedin.com/ashworks",
            "url": "https://github.com/Kiwis01/SparkyAI",
            "created_at": "2025-03-28T23:54:28Z",
            "updated_at": "2025-04-10T02:23:29Z",
            "pushed_at": "2025-04-10T02:25:59Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "TumorDetector",
            "description": "Tumor detection web app built upon flask",
            "language": "Python",
            "readme": "\ud83e\udde0 TumorDetector\nThis application is running on Flask and Python --version = Python 3.9.13 (There may be compatibility issues, any Python version 3.9 should work)\nSetup\nPlease make sure to install the required dependencies\npip install -qr requirements.txt \nTest cases\nIncluded 7 Images to try out the model, but feel free to use any images at your disposal\npath = ./static/prediction_examples\nDisclaimer\nThis code utilizes ultralytics yolov5 pretrained model for the training and prediction of brain tumor dataset. This model is still undergoing training, however updates will keep being made so stay tuned for that. Here is a link to check out their repository: https://github.com/ultralytics/yolov5\nPossible Issues\nThis pretrained model is only compatible for unix type devices (ex. MacOS and Linux/Ubuntu). To view on Windows devices it is needed to retrain the model yourself with my data. This issue is being solved and will be updated next.",
            "url": "https://github.com/Kiwis01/TumorDetector",
            "created_at": "2025-01-11T17:49:02Z",
            "updated_at": "2025-03-14T09:38:23Z",
            "pushed_at": "2025-01-13T21:59:17Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "Verso-Landing",
            "description": "Verso landing page still in dev",
            "language": "HTML",
            "readme": "VersoLandingPage\nVerso landing page",
            "url": "https://github.com/Kiwis01/Verso-Landing",
            "created_at": "2024-07-01T00:26:39Z",
            "updated_at": "2024-07-01T00:30:09Z",
            "pushed_at": "2024-07-01T00:30:07Z",
            "stargazers_count": 0,
            "forks_count": 0
        },
        {
            "name": "VersoJs",
            "description": "Verso Transcription from streamlit/python to html/js",
            "language": "HTML",
            "readme": "Verso App\nTranscription of Verso into html/js format\nUsage\nUtilized Vite for testing on local\nInstallations\nnpm install langchain @langchain/openai @langchain/core @langchain/community pdfjs-dist",
            "url": "https://github.com/Kiwis01/VersoJs",
            "created_at": "2024-07-12T08:13:50Z",
            "updated_at": "2024-09-10T13:18:12Z",
            "pushed_at": "2024-09-10T13:13:41Z",
            "stargazers_count": 0,
            "forks_count": 0
        }
    ]
}